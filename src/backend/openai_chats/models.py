from django.db import models
from django.conf import settings
from courses.models import ContentBlock
from django.core.cache import cache
from openai_chats.utils import count_tokens


ROLE_USER = "user"
ROLE_ASSISTANT = "assistant"
ROLE_SYSTEM = "system"
ROLE_DEVELOPER = "developer"


class Chat(models.Model):
    """This model stores chats."""

    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE, related_name="chats"
    )
    content_block = models.ForeignKey(
        ContentBlock, on_delete=models.CASCADE, related_name="chats"
    )
    token_input = models.PositiveIntegerField(
        default=0, help_text="Number of tokens used in the input messages"
    )
    token_output = models.PositiveIntegerField(
        default=0, help_text="Number of tokens used in the output messages"
    )
    created_at = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return f"{self.user.username} | {self.content_block.title}"

    @property
    def total_token_output(self):
        """
        Calculate total token count for assistant messages in this chat.
        This represents tokens generated by the AI (output from OpenAI's perspective).
        """
        return self.messages.filter(role=ROLE_ASSISTANT).aggregate(
            total=models.Sum('token_count')
        )['total'] or 0

    @property
    def total_token_input(self):
        """
        Calculate total token count for non-assistant messages in this chat.
        This represents tokens sent by users and system (input to OpenAI).
        """
        return self.messages.exclude(role=ROLE_ASSISTANT).aggregate(
            total=models.Sum('token_count')
        )['total'] or 0


class ChatMessage(models.Model):
    """This model stores messages in a chat."""

    ROLE_CHOICES = [
        (ROLE_USER, "User"),
        (ROLE_ASSISTANT, "Assistant"),
        (ROLE_SYSTEM, "System"),
        (ROLE_DEVELOPER, "Developer"),
    ]

    chat = models.ForeignKey(Chat, on_delete=models.CASCADE, related_name="messages")
    role = models.CharField(max_length=50, choices=ROLE_CHOICES)
    content = models.TextField()
    token_count = models.PositiveIntegerField(
        default=0, help_text="Number of tokens used in this message"
    )
    model_used = models.CharField(
        max_length=100,
        blank=True,
        null=True,
        help_text="Model used for generating this message",
    )
    created_at = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return f"{self.get_role_display()} | {self.content[:30]}"

    def save(self, *args, **kwargs):
        if not self.token_count:
            # Calculate token count based on content
            self.token_count = count_tokens(self.content, model_name=self.model_used)
        super().save(*args, **kwargs)


class Option(models.Model):
    """This model stores options for a chat message."""

    CACHE_KEY = "chat_options"
    CACHE_GLOBAL_PROMPT_KEY = "chat_global_prompt"
    CACHE_IS_ACTIVE_LESSON_CONTEXT_KEY = "chat_is_active_lesson_context"

    parameters = models.JSONField(default=dict, blank=True)
    is_active_lesson_context = models.BooleanField(
        default=True,
        help_text="If True, the chat will use lesson context for generating responses.",
    )
    global_prompt = models.TextField(
        blank=True,
        null=True,
        help_text="Global prompt for the chat. This will be used as a system message.",
    )

    def save(self, *args, **kwargs):
        cache.delete(self.CACHE_KEY)
        cache.delete(self.CACHE_GLOBAL_PROMPT_KEY)
        cache.delete(self.CACHE_IS_ACTIVE_LESSON_CONTEXT_KEY)
        super().save(*args, **kwargs)

    @classmethod
    def get_params(cls, default=None):
        """Get default options."""
        cache_key = cls.CACHE_KEY
        params: dict = cache.get(cache_key)
        if params is None:
            option = cls.objects.first()
            if option:
                params = option.parameters
            else:
                params = default or {}
            cache.set(cache_key, params)
        return params

    @classmethod
    def get_global_prompt(cls):
        """Get global prompt."""
        cache_key = cls.CACHE_GLOBAL_PROMPT_KEY
        global_prompt: str = cache.get(cache_key)
        if global_prompt is None:
            option = cls.objects.first()
            if option:
                global_prompt = option.global_prompt
            else:
                global_prompt = ""
            cache.set(cache_key, global_prompt)
        return global_prompt

    @classmethod
    async def aget_is_active_lesson_context(cls) -> bool:
        """Asynchronously get if lesson context is active, using cache and DB."""
        cache_key = cls.CACHE_IS_ACTIVE_LESSON_CONTEXT_KEY

        # пытаемся достать из кэша
        is_active = await cache.aget(cache_key)
        if is_active is None:
            # пытаемся достать из кэша
            option = await cls.objects.afirst()
            if option:
                is_active = option.is_active_lesson_context
            else:
                is_active = False
            await cache.aset(cache_key, is_active)
        return is_active

    @classmethod
    async def aget_params(cls, default=None) -> dict:
        """Asynchronously get default options, using cache and DB."""
        cache_key = cls.CACHE_KEY

        # пытаемся достать из кэша
        params = await cache.aget(cache_key)
        if params is None:
            # пытаемся достать из кэша
            option = await cls.objects.afirst()
            if option:
                params = option.parameters
            else:
                params = default or {}
            cache.aset(cache_key, params)
        return params

    @classmethod
    async def aget_global_prompt(cls) -> str:
        """Asynchronously get global prompt, using cache and DB."""
        cache_key = cls.CACHE_GLOBAL_PROMPT_KEY

        # пытаемся достать из кэша
        global_prompt = await cache.aget(cache_key)
        if global_prompt is None:
            # пытаемся достать из кэша
            option = await cls.objects.afirst()
            if option:
                global_prompt = option.global_prompt
            else:
                global_prompt = ""
            cache.aset(cache_key, global_prompt)
        return global_prompt
